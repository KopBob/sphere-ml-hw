{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 4. Конструирование текстовых признаков из твитов пользователей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый этап - сбор твитов пользователей. Необходимо подключаться к Twitter API и запрашивать твиты по id пользователя. \n",
    "Подключение к API подробно описано в ДЗ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "CONSUMER_KEY = \"wTOj2975uKAhyIyJkrjod2wju\"\n",
    "CONSUMER_SECRET = \"v0IpKZ6LxnUN0DgGGAmLE7tgzjiK6u5zMYZnSIFX5ghN8hUoTp\"\n",
    "\n",
    "ACCESS_TOKEN_KEY = \"272608335-xS7I2zHOzMQHaI2eSO3RTsPVi8XwOOajXsMxLW1P\"\n",
    "ACCESS_TOKEN_SECRET = \"04aF4bmEktxhNv6jpi5QOESYIdLALUAiSmLJgs4h66q6H\"\n",
    "\n",
    "api = twitter.Api(consumer_key=CONSUMER_KEY, \n",
    "                  consumer_secret=CONSUMER_SECRET, \n",
    "                  access_token_key=ACCESS_TOKEN_KEY, \n",
    "                  access_token_secret=ACCESS_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Для получения твитов пользователя может быть использован метод GetUserTimeline из библиотеки python-twitter. Он позволяет получить не более 200 твитов пользователя.\n",
    "\n",
    "Метод имеет ограничение по количеству запросов в секунду. Для получения информации о промежутке времени, которое необходимо подождать для повторного обращения к API может быть использован метод `GetSleepTime`. Для получения информации об ограничениях запросов с помощью метода `GetUserTimeLine` необходимо вызывать `GetSleepTime` с параметром \"statuses/user_timeline\".\n",
    "\n",
    "Метод GetUserTimeline возвращает объекты типа Status. У этих объектов есть метод AsDict, который позволяет представить твит в виде словаря.\n",
    "\n",
    "Id пользователей необходимо считать из файла, как было сделано в ДЗ 1.\n",
    "\n",
    "Необходимо реализовать функцию `get_user_tweets(user_id)`. Входной параметр - id пользователя из файла. Возвращаемое значение - массив твитов пользователя, где каждый твит представлен в виде словаря. Предполагается, что информация о пользователе содержится в твитах, которые пользователь написал сам. Это означает, что можно попробовать отфильтровать ответы другим пользователям, ссылки и ретвиты, а так же картинки и видео, так как наша цель - найти текстовую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users twitts from files/twitts.json ...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def load_twitts(path):\n",
    "    print \"Reading users twitts from %s ...\" % path\n",
    "    with open(path, 'r') as infile:\n",
    "        twitts = json.load(infile)\n",
    "        return twitts\n",
    "\n",
    "twts = load_twitts('files/twitts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_tweets(user_id):\n",
    "    \"\"\"returns list of tweets as dicts\"\"\"\n",
    "    user_twts = twts.get(\"%s\" % user_id)\n",
    "    if not user_twts:\n",
    "        return []\n",
    "    data = {\n",
    "        'lang': 1,\n",
    "        'favorited': 1,\n",
    "        'truncated': 1,\n",
    "        'text': 1,\n",
    "        'created_at': 1,\n",
    "        'retweeted': 1,\n",
    "        'source': 1,\n",
    "        'user': 1,\n",
    "        'id': 1,\n",
    "    }\n",
    "\n",
    "    user_twts = [{key: value for key, value in twt.items() if key in data}\n",
    "                                               for twt in user_twts if twt]\n",
    "    return user_twts\n",
    "\n",
    "# def get_user_tweets(user_id):\n",
    "#     \"\"\"returns list of tweets as dicts\"\"\"\n",
    "#     fields = {\n",
    "#         'lang': 1,\n",
    "#         'favorited': 1,\n",
    "#         'truncated': 1,\n",
    "#         'text': 1,\n",
    "#         'created_at': 1,\n",
    "#         'retweeted': 1,\n",
    "#         'source': 1,\n",
    "#         'user': 1,\n",
    "#         'id': 1,\n",
    "#         '_id': 0\n",
    "#     }\n",
    "#     return list(shpere_db.twitts.find({u\"user_id\": u\"%s\" % user_id}, fields))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбор текста твита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка текста предполагает разбиение текста на отдельные элементы - параграфы, предложения, слова. Мы будем преобразовывать текст твита к словам. Для этого текст необходимо разбить на слова. Сделать это можно, например, с помощью регулярного выражения.\n",
    "\n",
    "Необходимо реализовать функцию, `get_words(text)`. Входной параметр - строка с текстом. Возвращаемое значение - массив строк (слов). Обратите внимание, что нужно учесть возможное наличие пунктуации и выделить по возможности только слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def get_words(text):\n",
    "    \"\"\"returns list of words\"\"\"\n",
    "    return [word.lower() for word in tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'different', 'words']\n"
     ]
    }
   ],
   "source": [
    "print get_words(\"Here are different words!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее полученные слова необходимо привести к нормальной форме. То есть привести их к форме единственного числа настоящего времени и пр. Сделать это можно с помощью библиотеки nltk. Информацию по загрузке, установке библиотеки и примерах использования можно найти на сайте http://www.nltk.org/\n",
    "\n",
    "Для загрузки всех необходимых словарей можно воспользоваться методом download из библиотеки nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xДля дальнейшей обработки слова должны быть приведены к нижнему регистру. \n",
    "\n",
    "Для приведения к нормальной форме можно использовать `WordNetLemmatizer` из библиотеки nltk. У этого класса есть метод `lemmatize`.\n",
    "\n",
    "Также необходимо убрать из текста так называемые стоп-слова. Это часто используемые слова, не несущие смысловой нагрузки для наших задач. Сделать это можно с помощью `stopwords` из nltk.corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tokens(words)`. Входной параметр - массив слов. Возвращаемое значение - массив токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_tokens(words):\n",
    "    \"\"\"returns list of tokens\"\"\"\n",
    "    words = [wnl.lemmatize(token) for token in words]\n",
    "    filtered_words = [word for word in words if word not in [u'http', u'co', u't', u'rt'] + stopwords.words('english')]\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['different', u'word']\n"
     ]
    }
   ],
   "source": [
    "print get_tokens([\"here\", \"are\", \"different\", \"words\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `get_tweet_tokens(tweet)`. Входной параметр - текст твита. Возвращаемое значение -- токены твита. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweet_tokens(tweet):\n",
    "    words = get_words(tweet)\n",
    "    return get_tokens(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо реализовать функцию `collect_users_tokens()`. Функция должна сконструировать матрицу признаков пользователей. В этой матрице строка - пользователь. Столбец - токен. На пересечении - сколько раз токен встречается у пользователя.\n",
    "Для построения матрицы можно использовать `DictVectorizer` из `sklearn.feature_extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def collect_users_tokens(df_users):\n",
    "    \"\"\"returns users list and list of user dicts. Each dict contains frequence of user tokens\"\"\"\n",
    "    tokens_counters = []\n",
    "    length = len(df_users.index)\n",
    "    for i, twitter_id in enumerate(df_users.twitter_id.tolist()):\n",
    "        sys.stdout.write('\\r' + \"%s/%s Precessing user_id=%s.\" % (i, length, twitter_id))\n",
    "        sys.stdout.flush()\n",
    "        tweets = get_user_tweets(twitter_id)\n",
    "        texts = [tweet.get(\"text\") for tweet in tweets]\n",
    "        merged_text = ' '.join(texts)\n",
    "        \n",
    "        tokens = get_tweet_tokens(merged_text)\n",
    "        tokens_counters.append(Counter(tokens)) #.most_common()[:10])\n",
    "        \n",
    "    return df_users.twitter_id.tolist(), tokens_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collect_users_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1a8c2773663d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0musers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musers_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_users_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_users\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collect_users_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\n",
    "TRAINING_SET_URL = \"../hw1/twitter_train.csv\"\n",
    "df_users = pd.read_csv(TRAINING_SET_URL, sep=\",\", header=True, names=[\"twitter_id\", \"is_1\", \"is_2\", \"is_3\"])\n",
    "    \n",
    "   \n",
    "    \n",
    "users, users_tokens = collect_users_tokens(df_users)\n",
    "\n",
    "v = DictVectorizer()\n",
    "vs = v.fit_transform(users_tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Сохраним полученные данные в файл. Используется метод savez из numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savez(\"files/out_4.dat\", data=vs, users=users, users_tokens=users_tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('files/out_4.dat.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_tokens = data[\"users_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_counts = sum((x for x in users_tokens.tolist()), Counter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее для получения представления о полученной информацию о токенах предлагается отобразить ее в виде облака тэгов. [Подсказка](http://anokhin.github.io/img/tag_cloud.png). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytagcloud import create_tag_image, make_tags\n",
    "from pytagcloud.lang.counter import get_tag_counts\n",
    "\n",
    "def draw_tag_cloud(tag_counts):\n",
    "    \"\"\"Draws tag cloud of found tokens\"\"\"\n",
    "    tags = make_tags(tag_counts, maxsize=120)\n",
    "    create_tag_image(tags, './files/cloud_large.png', size=(900, 600), fontname='Lobster')\n",
    "\n",
    "draw_tag_cloud(tag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
